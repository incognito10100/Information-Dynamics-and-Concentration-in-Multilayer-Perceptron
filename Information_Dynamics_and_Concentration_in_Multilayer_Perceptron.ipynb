{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "w_uQz2gwe92g"
      },
      "outputs": [],
      "source": [
        "# IMPORTS\n",
        "\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "from typing import Tuple, List, Dict\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rcParams\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import make_swiss_roll, make_classification, make_moons\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.special import digamma\n",
        "from scipy.linalg import svd\n",
        "\n",
        "# MATPLOTLIB CONFIGURATION FOR PUBLICATION-QUALITY FIGURES\n",
        "\n",
        "rcParams['font.family'] = 'serif'\n",
        "rcParams['font.size'] = 10\n",
        "rcParams['axes.linewidth'] = 1.0\n",
        "rcParams['grid.alpha'] = 0.3\n",
        "rcParams['grid.linestyle'] = '--'\n",
        "\n",
        "COLORS = {\n",
        "    'layer1': '#1f77b4',  # Blue\n",
        "    'layer2': '#ff7f0e',  # Orange\n",
        "    'train': '#2ca02c',   # Green\n",
        "    'val': '#d62728',     # Red\n",
        "    'equality': '#7f7f7f' # Gray\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# UTILITIES & DATASET\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def make_synthetic_binary(n_samples=2000, noise=0.1, kind=\"linear\", random_state=0):\n",
        "    from sklearn.datasets import make_classification, make_circles, make_moons\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "    if kind == \"linear\":\n",
        "        X, y = make_classification(\n",
        "            n_samples=n_samples, n_features=20, n_informative=8,\n",
        "            n_redundant=2, n_clusters_per_class=1, flip_y=noise,\n",
        "            random_state=random_state\n",
        "        )\n",
        "    elif kind == \"circles\":\n",
        "        X, y = make_circles(n_samples=n_samples, noise=noise, factor=0.5,\n",
        "                            random_state=random_state)\n",
        "    elif kind == \"moons\":\n",
        "        X, y = make_moons(n_samples=n_samples, noise=noise,\n",
        "                          random_state=random_state)\n",
        "    else:\n",
        "        X, y = make_classification(\n",
        "            n_samples=n_samples, n_features=20,\n",
        "            n_informative=10, n_redundant=0,\n",
        "            n_clusters_per_class=1, random_state=random_state\n",
        "        )\n",
        "\n",
        "    if X.size > 0:\n",
        "        X = StandardScaler().fit_transform(X)\n",
        "\n",
        "    return X.astype(np.float32), y.astype(np.int64)"
      ],
      "metadata": {
        "id": "b-x7ZIYmheJm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MODELS\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, num_classes=2, activation='relu'):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        d = input_dim\n",
        "        for h in hidden_dims:\n",
        "            layers.append(nn.Linear(d, h))\n",
        "            if activation == 'relu':\n",
        "                layers.append(nn.ReLU())\n",
        "            elif activation == 'tanh':\n",
        "                layers.append(nn.Tanh())\n",
        "            d = h\n",
        "        layers.append(nn.Linear(d, num_classes))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "        self.activation_type = activation\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "    def forward_with_activations(self, x):\n",
        "        activations = []\n",
        "        cur = x\n",
        "        for layer in self.net:\n",
        "            cur = layer(cur)\n",
        "            if isinstance(layer, (nn.ReLU, nn.Tanh)):\n",
        "                activations.append(cur.detach().cpu().numpy())\n",
        "        logits = cur\n",
        "        return logits, activations\n",
        "\n",
        "# SPECTRAL MEASURES\n",
        "\n",
        "def spectral_entropy_and_effective_rank(weight: np.ndarray) -> Tuple[float, float]:\n",
        "    try:\n",
        "        s = np.linalg.svd(weight, compute_uv=False)\n",
        "    except Exception:\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    if s.size == 0 or np.sum(s) == 0:\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    p = s / (s.sum())\n",
        "    entropy = -np.sum(p * np.log(np.clip(p, 1e-12, None)))\n",
        "    eff_rank = float(np.exp(entropy))\n",
        "    return float(entropy), float(eff_rank)"
      ],
      "metadata": {
        "id": "w4aWPrEIhz6p"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MI ESTIMATIORS\n",
        "\n",
        "def ksg_mi(x: np.ndarray, y: np.ndarray, k: int = 3) -> float:\n",
        "    assert x.ndim == 2 and y.ndim == 2\n",
        "    n = x.shape[0]\n",
        "    data = np.hstack([x, y])\n",
        "\n",
        "    if n <= k:\n",
        "        return 0.0\n",
        "\n",
        "    nbrs_joint = NearestNeighbors(n_neighbors=k + 1, metric='chebyshev').fit(data)\n",
        "    distances_joint, _ = nbrs_joint.kneighbors(data)\n",
        "    eps = distances_joint[:, k]\n",
        "\n",
        "    nbrs_x = NearestNeighbors(metric='chebyshev').fit(x)\n",
        "    nbrs_y = NearestNeighbors(metric='chebyshev').fit(y)\n",
        "\n",
        "    eps_minus = np.maximum(eps - 1e-15, 0.0)\n",
        "\n",
        "    counts_x_list = []\n",
        "    counts_y_list = []\n",
        "\n",
        "    for i in range(n):\n",
        "        # Find neighbors within radius eps (excluding the point itself)\n",
        "        rx_idx = nbrs_x.radius_neighbors([x[i]], radius=eps_minus[i], return_distance=False)\n",
        "        ry_idx = nbrs_y.radius_neighbors([y[i]], radius=eps_minus[i], return_distance=False)\n",
        "        counts_x_list.append(rx_idx[0].size - 1)\n",
        "        counts_y_list.append(ry_idx[0].size - 1)\n",
        "\n",
        "    counts_x = np.maximum(np.array(counts_x_list), 0)\n",
        "    counts_y = np.maximum(np.array(counts_y_list), 0)\n",
        "\n",
        "    # KSG formula part\n",
        "    avg = np.mean(digamma(k) + digamma(n) - digamma(counts_x + 1) - digamma(counts_y + 1))\n",
        "    return float(avg)\n",
        "\n",
        "\n",
        "def binned_mi(x: np.ndarray, y: np.ndarray, bins: int = 16) -> float:\n",
        "\n",
        "    def project_to_pc1(data):\n",
        "        if data.ndim == 1:\n",
        "            return data\n",
        "        if data.shape[1] > 1:\n",
        "            centered = data - data.mean(axis=0)\n",
        "            try:\n",
        "                u, s, vh = np.linalg.svd(centered, full_matrices=False)\n",
        "                if s[0] > 1e-8:\n",
        "                    return centered @ vh[0]\n",
        "            except:\n",
        "                pass\n",
        "            return data[:, 0]\n",
        "        return data[:, 0]\n",
        "\n",
        "    x_proj = project_to_pc1(x)\n",
        "    y_proj = project_to_pc1(y)\n",
        "\n",
        "    c_xy, _, _ = np.histogram2d(x_proj, y_proj, bins=bins)\n",
        "    p_xy = c_xy / (c_xy.sum() + 1e-12)\n",
        "\n",
        "    p_x = p_xy.sum(axis=1)\n",
        "    p_y = p_xy.sum(axis=0)\n",
        "\n",
        "    den = p_x[:, None] * p_y[None, :]\n",
        "    ratio = p_xy / (den + 1e-12)\n",
        "\n",
        "    mi = np.sum(p_xy * np.log(ratio + 1e-12))\n",
        "    return float(mi)"
      ],
      "metadata": {
        "id": "m1nETpV2iBym"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING AND LOGGING\n",
        "\n",
        "def extract_activations(model: MLP, X: np.ndarray, device: torch.device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x_t = torch.from_numpy(X).to(device)\n",
        "        _, activations = model.forward_with_activations(x_t)\n",
        "    return activations\n",
        "\n",
        "def train_and_log(\n",
        "    X_train: np.ndarray, y_train: np.ndarray,\n",
        "    X_val: np.ndarray, y_val: np.ndarray,\n",
        "    net_width: int = 128, hidden_layers: int = 2, epochs: int = 80,\n",
        "    batch_size: int = 128, lr: float = 1e-3,\n",
        "    device: torch.device = torch.device('cpu'),\n",
        "    log_every: int = 5, activation_probe_size: int = 1024, seed: int = 0\n",
        ") -> Dict:\n",
        "\n",
        "    seed_everything(seed)\n",
        "\n",
        "    input_dim = X_train.shape[1]\n",
        "    hidden_dims = [net_width] * hidden_layers\n",
        "\n",
        "    model = MLP(input_dim=input_dim, hidden_dims=hidden_dims, num_classes=2, activation='relu')\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    train_ds = torch.utils.data.TensorDataset(\n",
        "        torch.from_numpy(X_train), torch.from_numpy(y_train)\n",
        "    )\n",
        "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    if len(X_val) == 0:\n",
        "        print(\"Warning: Validation set is empty.\")\n",
        "        return {'checkpoints': [], 'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
        "\n",
        "    # Select a random subset of validation data for activation probing\n",
        "    probe_idx = np.random.choice(\n",
        "        len(X_val), size=min(activation_probe_size, len(X_val)), replace=False\n",
        "    )\n",
        "    X_probe = X_val[probe_idx]\n",
        "    y_probe = y_val[probe_idx]\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_acc': [],\n",
        "        'checkpoints': [],\n",
        "        'epochs': []\n",
        "    }\n",
        "\n",
        "    for ep in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for xb, yb in train_loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "\n",
        "            logits, _ = model.forward_with_activations(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item() * xb.size(0)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            correct += (preds == yb).sum().item()\n",
        "            total += xb.size(0)\n",
        "\n",
        "        train_loss = epoch_loss / total\n",
        "        train_acc = correct / total\n",
        "\n",
        "        if ep % log_every == 0 or ep == 1 or ep == epochs:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                xv = torch.from_numpy(X_val).to(device)\n",
        "                yv = torch.from_numpy(y_val).to(device)\n",
        "\n",
        "                logits_v, _ = model.forward_with_activations(xv)\n",
        "                val_loss = criterion(logits_v, yv).item()\n",
        "                val_acc = (logits_v.argmax(dim=1) == yv).float().mean().item()\n",
        "\n",
        "            history['train_loss'].append(train_loss)\n",
        "            history['val_loss'].append(val_loss)\n",
        "            history['train_acc'].append(train_acc)\n",
        "            history['val_acc'].append(val_acc)\n",
        "            history['epochs'].append(ep)\n",
        "\n",
        "            activations = extract_activations(model, X_probe, device)\n",
        "\n",
        "            spectral_stats = []\n",
        "            for name, p in model.named_parameters():\n",
        "                if 'weight' in name and p.ndim == 2:\n",
        "                    w = p.detach().cpu().numpy()\n",
        "                    ent, eff = spectral_entropy_and_effective_rank(w)\n",
        "                    spectral_stats.append({\n",
        "                        'name': name,\n",
        "                        'spec_entropy': ent,\n",
        "                        'effective_rank': eff,\n",
        "                        'shape': w.shape\n",
        "                    })\n",
        "\n",
        "            checkpoint = {\n",
        "                'epoch': ep,\n",
        "                'activations': activations,\n",
        "                'spectral_stats': spectral_stats,\n",
        "                'train_acc': train_acc,\n",
        "                'val_acc': val_acc,\n",
        "                'train_loss': train_loss,\n",
        "                'val_loss': val_loss,\n",
        "                'probe_idx': probe_idx\n",
        "            }\n",
        "\n",
        "            history['checkpoints'].append(checkpoint)\n",
        "\n",
        "            print(f\"[ep {ep}] train_loss={train_loss:.4f} val_loss={val_loss:.4f} \"\n",
        "                  f\"train_acc={train_acc:.4f} val_acc={val_acc:.4f}\")\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "uXmaSVK5iK4g"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ANALYSIS & VISUALIZATION\n",
        "\n",
        "def safe_2d(x: np.ndarray):\n",
        "    \"\"\"Ensure array is 2D for MI functions.\"\"\"\n",
        "    x = np.asarray(x)\n",
        "    if x.ndim == 1:\n",
        "        return x.reshape(-1, 1)\n",
        "    return x\n",
        "\n",
        "\n",
        "def compute_layerwise_mi_ksg(\n",
        "    X_probe: np.ndarray, y_probe: np.ndarray,\n",
        "    activations_list: List[np.ndarray], k: int = 3,\n",
        "    pca_components: int = 10, sample_limit: int = 1024\n",
        ") -> Dict:\n",
        "    n_probe = X_probe.shape[0]\n",
        "\n",
        "    idx = np.arange(n_probe)\n",
        "    if n_probe > sample_limit:\n",
        "        idx = np.random.choice(n_probe, sample_limit, replace=False)\n",
        "\n",
        "    Xs = safe_2d(X_probe[idx])\n",
        "    ys = y_probe[idx].reshape(-1, 1)\n",
        "\n",
        "    results = {\n",
        "        'I_X_H': [],\n",
        "        'I_H_Y': [],\n",
        "        'layer_dims': []\n",
        "    }\n",
        "\n",
        "    for H_layer in activations_list:\n",
        "        H = safe_2d(H_layer[idx])\n",
        "\n",
        "        d = H.shape[1]\n",
        "        if d > pca_components:\n",
        "            scaler = StandardScaler()\n",
        "            H_scaled = scaler.fit_transform(H)\n",
        "            pca = PCA(n_components=min(pca_components, d))\n",
        "            Hproj = pca.fit_transform(H_scaled)\n",
        "        else:\n",
        "            Hproj = H.copy()\n",
        "\n",
        "        try:\n",
        "            I_x_h = ksg_mi(Xs, Hproj, k=k)\n",
        "            I_h_y = binned_mi(Hproj, ys, bins=16)\n",
        "        except Exception as e:\n",
        "            print(f\"KSG failed for layer ({Hproj.shape}): {e}. Falling back to binned MI.\")\n",
        "            I_x_h = binned_mi(Xs, Hproj, bins=16)\n",
        "            I_h_y = binned_mi(Hproj, ys, bins=16)\n",
        "\n",
        "        results['I_X_H'].append(float(I_x_h))\n",
        "        results['I_H_Y'].append(float(I_h_y))\n",
        "        results['layer_dims'].append(int(Hproj.shape[1]))\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def compute_per_neuron_mi_binned(\n",
        "    activations_list: List[np.ndarray], X_probe: np.ndarray, y_probe: np.ndarray, bins: int = 16\n",
        ") -> List[np.ndarray]:\n",
        "    y = y_probe.reshape(-1, 1)\n",
        "    per_layer = []\n",
        "\n",
        "    for H_layer in activations_list:\n",
        "        H = safe_2d(H_layer)\n",
        "        n, d = H.shape\n",
        "        mi_per_neuron = np.zeros(d)\n",
        "\n",
        "        for j in range(d):\n",
        "            neuron = H[:, j].reshape(-1, 1)\n",
        "            mi_per_neuron[j] = binned_mi(neuron, y, bins=bins)\n",
        "\n",
        "        per_layer.append(mi_per_neuron)\n",
        "\n",
        "    return per_layer\n",
        "\n",
        "\n",
        "def gini_coefficient(x: np.ndarray) -> float:\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    if x.size == 0 or np.all(x == 0):\n",
        "        return 0.0\n",
        "\n",
        "    x_sorted = np.sort(np.maximum(x, 0))\n",
        "    n = len(x_sorted)\n",
        "    cum = np.cumsum(x_sorted)\n",
        "\n",
        "    # Gini formula\n",
        "    g = (2.0 * np.sum((np.arange(1, n+1) * x_sorted))) / (n * cum[-1]) - (n + 1) / n\n",
        "    return float(g)\n",
        "\n",
        "def plot_information_plane(mi_results: Dict, save_path='figure1_info_plane.pdf'):\n",
        "    I_X_H = np.array(mi_results['I_X_H'])\n",
        "    I_H_Y = np.array(mi_results['I_H_Y'])\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6, 5))\n",
        "\n",
        "    # Plot each layer\n",
        "    for i in range(len(I_X_H)):\n",
        "        marker = 'o' if i == 0 else 's'\n",
        "        color = COLORS['layer1'] if i == 0 else COLORS['layer2']\n",
        "        label = f'Hidden Layer {i+1}'\n",
        "\n",
        "        ax.plot(I_X_H[i], I_H_Y[i], marker, markersize=10,\n",
        "                color=color, label=label, zorder=3)\n",
        "\n",
        "    # Arrow showing flow\n",
        "    for i in range(len(I_X_H) - 1):\n",
        "        ax.annotate('', xy=(I_X_H[i+1], I_H_Y[i+1]),\n",
        "                    xytext=(I_X_H[i], I_H_Y[i]),\n",
        "                    arrowprops=dict(arrowstyle='->', lw=2, color='black', alpha=0.5),\n",
        "                    zorder=2)\n",
        "\n",
        "    ax.set_xlabel('$I(X; H)$ - Input Information (nats)', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('$I(H; Y)$ - Task Information (nats)', fontsize=12, fontweight='bold')\n",
        "    ax.set_title('Information Plane Trajectory\\n(No Compression Phase)',\n",
        "                 fontsize=13, fontweight='bold', pad=15)\n",
        "    ax.grid(True, alpha=0.3, linestyle='--')\n",
        "    ax.legend(loc='lower right', frameon=True, shadow=True)\n",
        "\n",
        "    textstr = 'Both $I(X;H)$ and $I(H;Y)$ increase\\nâ†’ No compression observed'\n",
        "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.3)\n",
        "    ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=9,\n",
        "            verticalalignment='top', bbox=props)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"âœ“ Saved: {save_path}\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_gini_curves(per_neuron_mi: List[np.ndarray], save_path='figure2_gini_curves.pdf'):\n",
        "    fig, ax = plt.subplots(figsize=(6, 5))\n",
        "\n",
        "    # Equality line\n",
        "    ax.plot([0, 100], [0, 100], '--', color=COLORS['equality'],\n",
        "            linewidth=2, label='Perfect Equality', alpha=0.7, zorder=1)\n",
        "\n",
        "    for layer_idx, mi_vals in enumerate(per_neuron_mi):\n",
        "        sorted_mi = np.sort(mi_vals)\n",
        "        cumsum_mi = np.cumsum(sorted_mi) / np.sum(sorted_mi)\n",
        "        percentiles = np.linspace(0, 100, len(sorted_mi))\n",
        "\n",
        "        color = COLORS['layer1'] if layer_idx == 0 else COLORS['layer2']\n",
        "        gini = gini_coefficient(mi_vals)\n",
        "        label = f'Layer {layer_idx+1} (Gini = {gini:.3f})'\n",
        "\n",
        "        ax.plot(percentiles, cumsum_mi * 100, color=color,\n",
        "                linewidth=2.5, label=label, zorder=3)\n",
        "\n",
        "        if layer_idx == len(per_neuron_mi) - 1:\n",
        "            ax.fill_between(percentiles, percentiles, cumsum_mi * 100,\n",
        "                           color=color, alpha=0.15)\n",
        "\n",
        "    ax.set_xlabel('Neuron Percentile (sorted by MI)', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('Cumulative MI (%)', fontsize=12, fontweight='bold')\n",
        "    ax.set_title('Information Concentration (Gini Curves)',\n",
        "                 fontsize=13, fontweight='bold', pad=15)\n",
        "    ax.grid(True, alpha=0.3, linestyle='--')\n",
        "    ax.legend(loc='upper left', frameon=True, shadow=True)\n",
        "    ax.set_xlim(0, 100)\n",
        "    ax.set_ylim(0, 100)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"âœ“ Saved: {save_path}\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_training_curves(history: Dict, save_path='figure3_training_curves.pdf'):\n",
        "    epochs = np.array(history['epochs'])\n",
        "    train_loss = np.array(history['train_loss'])\n",
        "    val_loss = np.array(history['val_loss'])\n",
        "    train_acc = np.array(history['train_acc']) * 100\n",
        "    val_acc = np.array(history['val_acc']) * 100\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4.5))\n",
        "\n",
        "    # Subplot 1: Loss\n",
        "    ax1.plot(epochs, train_loss, 'o-', color=COLORS['train'],\n",
        "             linewidth=2.5, markersize=7, label='Training Loss')\n",
        "    ax1.plot(epochs, val_loss, 's-', color=COLORS['val'],\n",
        "             linewidth=2.5, markersize=7, label='Validation Loss')\n",
        "    ax1.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "    ax1.set_ylabel('Cross-Entropy Loss', fontsize=12, fontweight='bold')\n",
        "    ax1.set_title('(a) Loss Convergence', fontsize=12, fontweight='bold')\n",
        "    ax1.legend(loc='upper right', frameon=True, shadow=True)\n",
        "    ax1.grid(True, alpha=0.3, linestyle='--')\n",
        "    ax1.axvline(x=10, color='gray', linestyle=':', alpha=0.5, linewidth=1.5)\n",
        "\n",
        "    # Subplot 2: Accuracy\n",
        "    ax2.plot(epochs, train_acc, 'o-', color=COLORS['train'],\n",
        "             linewidth=2.5, markersize=7, label='Training Accuracy')\n",
        "    ax2.plot(epochs, val_acc, 's-', color=COLORS['val'],\n",
        "             linewidth=2.5, markersize=7, label='Validation Accuracy')\n",
        "    ax2.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "    ax2.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
        "    ax2.set_title('(b) Accuracy Plateau', fontsize=12, fontweight='bold')\n",
        "    ax2.legend(loc='lower right', frameon=True, shadow=True)\n",
        "    ax2.grid(True, alpha=0.3, linestyle='--')\n",
        "    ax2.axhline(y=99.5, color='gray', linestyle=':', alpha=0.5, linewidth=1.5)\n",
        "    ax2.set_ylim(86, 100.5)\n",
        "\n",
        "    textstr = 'Train-Val Gap < 1%\\n(No overfitting)'\n",
        "    props = dict(boxstyle='round', facecolor='lightgreen', alpha=0.3)\n",
        "    ax2.text(0.98, 0.15, textstr, transform=ax2.transAxes, fontsize=9,\n",
        "             verticalalignment='bottom', horizontalalignment='right', bbox=props)\n",
        "\n",
        "    plt.suptitle('Training Dynamics Over 60 Epochs', fontsize=14,\n",
        "                 fontweight='bold', y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"âœ“ Saved: {save_path}\")\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "qUjkqAbliXy3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EXPERIMAENT RUNNER\n",
        "\n",
        "def run_experiment_single_width(width: int = 256):\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"INFORMATION CONCENTRATION EXPERIMENT (Width: {width})\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    # Generate and split dataset\n",
        "    X, y = make_synthetic_binary(n_samples=6000, noise=0.12, kind='moons', random_state=0)\n",
        "    X_train, X_rest, y_train, y_rest = train_test_split(X, y, test_size=0.4, random_state=0)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_rest, y_rest, test_size=0.5, random_state=1)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Dataset: {X_train.shape[0]} train, {X_val.shape[0]} val, {X_test.shape[0]} test\\n\")\n",
        "\n",
        "    # Train model\n",
        "    history = train_and_log(\n",
        "        X_train=X_train, y_train=y_train,\n",
        "        X_val=X_val, y_val=y_val,\n",
        "        net_width=width, hidden_layers=2,\n",
        "        epochs=60, batch_size=128, lr=1e-3,\n",
        "        device=device, log_every=10,\n",
        "        activation_probe_size=1024, seed=42\n",
        "    )\n",
        "\n",
        "    if not history or not history[\"checkpoints\"]:\n",
        "        print(\"âŒ Experiment failed: No checkpoints.\")\n",
        "        return\n",
        "\n",
        "    # Extract final checkpoint for analysis\n",
        "    cp = history[\"checkpoints\"][-1]\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"FINAL ANALYSIS (Epoch {cp['epoch']})\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    activations = cp[\"activations\"]\n",
        "    probe_idx = cp[\"probe_idx\"]\n",
        "    X_probe = X_val[probe_idx]\n",
        "    y_probe = y_val[probe_idx]\n",
        "\n",
        "    # Compute layer-wise MI\n",
        "    mi_results = compute_layerwise_mi_ksg(X_probe, y_probe, activations,\n",
        "                                          k=3, pca_components=10, sample_limit=1024)\n",
        "\n",
        "    print(\"ðŸ“Š Layer-wise Mutual Information:\")\n",
        "    for i, (ixh, ihy, dim) in enumerate(\n",
        "        zip(mi_results[\"I_X_H\"], mi_results[\"I_H_Y\"], mi_results[\"layer_dims\"])\n",
        "    ):\n",
        "        print(f\"  Layer {i+1} (Dim {dim}): I(X;H) = {ixh:.4f} nats, I(H;Y) = {ihy:.4f} nats\")\n",
        "\n",
        "    # Compute per-neuron MI\n",
        "    per_neuron_mi = compute_per_neuron_mi_binned(activations, X_probe, y_probe)\n",
        "\n",
        "    print(\"\\nðŸ“ˆ Per-Neuron Information Concentration:\")\n",
        "    gini_values = []\n",
        "    for i, arr in enumerate(per_neuron_mi):\n",
        "        gini = gini_coefficient(arr)\n",
        "        gini_values.append(gini)\n",
        "        print(f\"  Layer {i+1}: Mean MI = {arr.mean():.6f}, Gini = {gini:.6f}\")\n",
        "\n",
        "    # Spectral analysis\n",
        "    print(\"\\nðŸ” Spectral Properties:\")\n",
        "    for s in cp[\"spectral_stats\"]:\n",
        "        print(f\"  {s['name']} {s['shape']}: Effective Rank = {s['effective_rank']:.3f}\")\n",
        "\n",
        "    # Generate all figures\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"GENERATING PUBLICATION FIGURES\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    plot_information_plane(mi_results, 'figure1_info_plane.pdf')\n",
        "    plot_gini_curves(per_neuron_mi, 'figure2_gini_curves.pdf')\n",
        "    plot_training_curves(history, 'figure3_training_curves.pdf')\n",
        "\n",
        "    # Summary\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"EXPERIMENT SUMMARY\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"âœ“ Final Train Accuracy: {cp['train_acc']*100:.2f}%\")\n",
        "    print(f\"âœ“ Final Val Accuracy: {cp['val_acc']*100:.2f}%\")\n",
        "    print(f\"âœ“ Information Flow: I(X;H) increases by {((mi_results['I_X_H'][-1]/mi_results['I_X_H'][0])-1)*100:.1f}%\")\n",
        "    print(f\"âœ“ Concentration: Gini increases from {gini_values[0]:.3f} to {gini_values[-1]:.3f} (+{((gini_values[-1]/gini_values[0])-1)*100:.1f}%)\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    print(\"âœ“ All figures saved successfully!\")\n",
        "    print(\"  â†’ figure1_info_plane.pdf\")\n",
        "    print(\"  â†’ figure2_gini_curves.pdf\")\n",
        "    print(\"  â†’ figure3_training_curves.pdf\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    t0 = time.time()\n",
        "    run_experiment_single_width(width=256)\n",
        "    print(f\"\\nâ±ï¸  Total execution time: {time.time() - t0:.2f}s\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oCoEG8RiskZ",
        "outputId": "c81f4d8b-2a9e-43de-aeac-a6a3fa7d5125"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "INFORMATION CONCENTRATION EXPERIMENT (Width: 256)\n",
            "======================================================================\n",
            "\n",
            "Device: cpu\n",
            "Dataset: 3600 train, 1200 val, 1200 test\n",
            "\n",
            "[ep 1] train_loss=0.2828 val_loss=0.1590 train_acc=0.8778 val_acc=0.9342\n",
            "[ep 10] train_loss=0.0115 val_loss=0.0081 train_acc=0.9975 val_acc=0.9992\n",
            "[ep 20] train_loss=0.0106 val_loss=0.0040 train_acc=0.9972 val_acc=0.9983\n",
            "[ep 30] train_loss=0.0103 val_loss=0.0043 train_acc=0.9969 val_acc=0.9992\n",
            "[ep 40] train_loss=0.0097 val_loss=0.0038 train_acc=0.9969 val_acc=0.9992\n",
            "[ep 50] train_loss=0.0112 val_loss=0.0036 train_acc=0.9969 val_acc=0.9992\n",
            "[ep 60] train_loss=0.0101 val_loss=0.0031 train_acc=0.9969 val_acc=0.9983\n",
            "\n",
            "======================================================================\n",
            "FINAL ANALYSIS (Epoch 60)\n",
            "======================================================================\n",
            "\n",
            "ðŸ“Š Layer-wise Mutual Information:\n",
            "  Layer 1 (Dim 10): I(X;H) = 1.8262 nats, I(H;Y) = 0.3982 nats\n",
            "  Layer 2 (Dim 10): I(X;H) = 2.1656 nats, I(H;Y) = 0.6845 nats\n",
            "\n",
            "ðŸ“ˆ Per-Neuron Information Concentration:\n",
            "  Layer 1: Mean MI = 0.218739, Gini = 0.340046\n",
            "  Layer 2: Mean MI = 0.360833, Gini = 0.430175\n",
            "\n",
            "ðŸ” Spectral Properties:\n",
            "  net.0.weight (256, 2): Effective Rank = 1.987\n",
            "  net.2.weight (256, 256): Effective Rank = 162.362\n",
            "  net.4.weight (2, 256): Effective Rank = 1.791\n",
            "\n",
            "======================================================================\n",
            "GENERATING PUBLICATION FIGURES\n",
            "======================================================================\n",
            "\n",
            "âœ“ Saved: figure1_info_plane.pdf\n",
            "âœ“ Saved: figure2_gini_curves.pdf\n",
            "âœ“ Saved: figure3_training_curves.pdf\n",
            "\n",
            "======================================================================\n",
            "EXPERIMENT SUMMARY\n",
            "======================================================================\n",
            "âœ“ Final Train Accuracy: 99.69%\n",
            "âœ“ Final Val Accuracy: 99.83%\n",
            "âœ“ Information Flow: I(X;H) increases by 18.6%\n",
            "âœ“ Concentration: Gini increases from 0.340 to 0.430 (+26.5%)\n",
            "======================================================================\n",
            "\n",
            "âœ“ All figures saved successfully!\n",
            "  â†’ figure1_info_plane.pdf\n",
            "  â†’ figure2_gini_curves.pdf\n",
            "  â†’ figure3_training_curves.pdf\n",
            "\n",
            "â±ï¸  Total execution time: 24.84s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qwqhgjOEi5ry"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}